{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers[torch] tokenizers datasets evaluate rouge_score sentencepiece huggingface_hub --upgrade","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nfrom datasets import load_dataset\nimport evaluate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\nimport torch\nfrom nltk.corpus import stopwords\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom sklearn.metrics import f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading dataset\ndataset = load_dataset(\"toughdata/quora-question-answer-dataset\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Dataset structure: {dataset}\")\n\n# Analyzing the structure of the dataset\nprint(\"Dataset columns and sample data:\")\nprint(dataset[\"train\"].column_names)\nprint(dataset[\"train\"].features)\n\ndf = pd.DataFrame(dataset['train'])\n\n# Analyzing the structure and content of the dataset\nprint(\"Dataset structure:\")\nprint(df.head())\nprint(\"\\nDataset info:\")\nprint(df.info())\nprint(\"\\nDataset description:\")\nprint(df.describe())\n\n\n# Checking for missing values\nprint(\"\\nMissing Values in Data:\")\nprint(df.isnull().sum())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contractions_dict = {\n    \"ain't\": \"am not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he had / he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"i'd\": \"I had / I would\",\n    \"i'd've\": \"I would have\",\n    \"i'll\": \"I will\",\n    \"i'll've\": \"I will have\",\n    \"i'm\": \"I am\",\n    \"i've\": \"I have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it had / it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it has / it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she had / she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so is\",\n    \"that'd\": \"that had\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that has / that is\",\n    \"there'd\": \"there had / there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there has / there is\",\n    \"they'd\": \"they had / they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they shall / they will\",\n    \"they'll've\": \"they shall have / they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we had / we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what shall / what will\",\n    \"what'll've\": \"what shall have / what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what has / what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when has / when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where has / where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who shall / who will\",\n    \"who'll've\": \"who shall have / who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef expand_contractions(text, contractions_dict):\n    if not text:\n        return \"\"\n    for contraction, expansion in contractions_dict.items():\n        text = re.sub(r'\\b{}\\b'.format(re.escape(contraction)), expansion, text)\n    return text\n\n# Text preprocessing function\ndef text_preprocessing(text):\n    if pd.isnull(text):  # Checking if the text is NaN\n        return ''\n    \n    # Expanding contractions\n    text = expand_contractions(text, contractions_dict)\n    \n    # Converting text to lowercase\n    text = text.lower()\n    \n    # Removing HTML tags\n    text = re.sub(r'<.*?>', '', text)\n    \n    # Replacing non-alphabetic characters with spaces, but keep contractions intact\n    text = re.sub(r'[^a-z\\' ]', ' ', text)\n    \n    # Removing extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n# Function to preprocess texts\ndef preprocess_texts(df):\n    df['question'] = df['question'].apply(lambda x: text_preprocessing(x))\n    df['answer'] = df['answer'].apply(lambda x: text_preprocessing(x))\n    return df\n\ndf_train = preprocess_texts(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text = df_train['question'].iloc[7]\norig = df['question'].iloc[7]\nexpanded_text = expand_contractions(sample_text, contractions_dict)\nprint(f\"Original: {orig}\")\nprint(f\"Expanded: {expanded_text}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Visualization\n# Distribution of question lengths\ndf_train['question_length'] = df_train['question'].apply(lambda x: len(x.split()))\nplt.figure(figsize=(12, 6))\nsns.histplot(df_train['question_length'], bins=50, kde=True)\nplt.title('Distribution of Question Lengths')\nplt.xlabel('Length of Questions')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of answer lengths\ndf_train['answer_length'] = df_train['answer'].apply(lambda x: len(x.split()))\n\n# Handling possible infinite values\ndf_train = df_train.replace([np.inf, -np.inf], np.nan).dropna()\n\nplt.figure(figsize=(12, 6))\nsns.histplot(df_train['answer_length'], bins=50, kde=True)\nplt.title('Distribution of Answer Lengths')\nplt.xlabel('Length of Answers')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nfrom nltk.corpus import stopwords\n\n# Download stopwords\nnltk.download('stopwords')\n\n# Load stop words from nltk and wordcloud\nnltk_stopwords = set(stopwords.words('english'))\nwordcloud_stopwords = set(STOPWORDS)\n\n# Combining NLTK and WordCloud stop words\nstop_words = nltk_stopwords.union(wordcloud_stopwords)\n\n# Word cloud for questions\ntext = ' '.join(df_train['question'])\nwordcloud = WordCloud(stopwords=stop_words,width=800, height=400, background_color='white').generate(text)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud for Questions')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word cloud for answers\ntext = ' '.join(df_train['answer'])\nwordcloud = WordCloud(stopwords=stop_words,width=800, height=400, background_color='white').generate(text)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud for Answers')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(df_train))\nprint(type(dataset))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the dataset\nfrom datasets import Dataset\ndf_train = df_train[['question','answer']]\ndataset = Dataset.from_pandas(df_train)\ndataset = dataset.train_test_split(test_size=0.25)\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prefix = \"answer the question: \"\n\n# Preprocessing function for model\ndef preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"question\"]]\n    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n    labels = tokenizer(text_target=examples[\"answer\"], max_length=512, truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download(\"punkt\", quiet=True)\nmetric = evaluate.load(\"rouge\")\n\nfrom rouge_score import rouge_scorer\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom sklearn.metrics import f1_score\n\n# Initializing ROUGE scorer\nrouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    # Decoding predictions and labels\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Calculating ROUGE scores\n    rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n    for pred, label in zip(decoded_preds, decoded_labels):\n        scores = rouge_scorer.score(label, pred)\n        rouge_scores['rouge1'] += scores['rouge1'].fmeasure\n        rouge_scores['rouge2'] += scores['rouge2'].fmeasure\n        rouge_scores['rougeL'] += scores['rougeL'].fmeasure\n    \n    num_samples = len(decoded_preds)\n    rouge_scores = {key: score / num_samples for key, score in rouge_scores.items()}\n\n    # Calculating BLEU score\n    bleu_scores = [sentence_bleu([nltk.word_tokenize(label)], nltk.word_tokenize(pred)) for pred, label in zip(decoded_preds, decoded_labels)]\n    bleu_score = sum(bleu_scores) / len(bleu_scores)\n\n    # Calculating F1 score\n    f1_scores = f1_score(decoded_labels, decoded_preds, average='weighted', zero_division=0)\n\n    return {\n        \"rouge1\": rouge_scores['rouge1'],\n        \"rouge2\": rouge_scores['rouge2'],\n        \"rougeL\": rouge_scores['rougeL'],\n        \"bleu\": bleu_score,\n        \"f1\": f1_scores,\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    learning_rate=3e-4,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=4,\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=3,\n    predict_with_generate=True,\n    push_to_hub=False,\n    report_to=\"none\"\n)\n\n# Set up trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(\"./t5_fine_tuned\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_training_loss(log_history):\n    # Extract loss values and steps from log history\n    steps = [log['step'] for log in log_history if 'loss' in log]\n    losses = [log['loss'] for log in log_history if 'loss' in log]\n    \n    plt.figure(figsize=(12, 6))\n    plt.plot(steps, losses, label='Training Loss', color='blue')\n    plt.xlabel('Steps')\n    plt.ylabel('Loss')\n    plt.title('Training Loss Over Steps')\n    plt.legend()\n    plt.show()\n\n# After training, extract and plot training loss\nlog_history = trainer.state.log_history\nplot_training_loss(log_history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_training_loss(log_history):\n    # Extract loss values and epochs from log history\n    epochs = [log['epoch'] for log in log_history if 'loss' in log and 'epoch' in log]\n    losses = [log['loss'] for log in log_history if 'loss' in log and 'epoch' in log]\n    \n    plt.figure(figsize=(12, 6))\n    plt.plot(epochs, losses, marker='o', label='Training Loss', color='blue')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training Loss Over Epochs')\n    plt.legend()\n    plt.show()\n\n# Plloting training loss\nlog_history = trainer.state.log_history\nplot_training_loss(log_history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating the model to get the metrics\neval_results = trainer.evaluate()\n\nrouge_scores = {\n    'rouge1': eval_results['eval_rouge1'],\n    'rouge2': eval_results['eval_rouge2'],\n    'rougeL': eval_results['eval_rougeL']\n}\nbleu_score = eval_results['eval_bleu']\n\nimport matplotlib.pyplot as plt\n\n# Function to plot ROUGE scores\ndef plot_rouge_scores(rouge_scores):\n    plt.figure(figsize=(12, 6))\n    plt.bar(rouge_scores.keys(), rouge_scores.values(), color=['blue', 'orange', 'green'])\n    plt.title('ROUGE Scores')\n    plt.xlabel('ROUGE Type')\n    plt.ylabel('Score')\n    plt.ylim(0, 1)\n    plt.show()\n\n# Function to plot BLEU score\ndef plot_bleu_score(bleu_score):\n    plt.figure(figsize=(8, 4))\n    plt.bar(['BLEU Score'], [bleu_score], color='purple')\n    plt.title('BLEU Score')\n    plt.xlabel('Metric')\n    plt.ylabel('Score')\n    plt.ylim(0, 1)\n    plt.show()\n\n# Plot the metrics\nplot_rouge_scores(rouge_scores)\nplot_bleu_score(bleu_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(eval_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prefix = \"answer the question: \"\nquestion = \"How can I increase sales?\"\n\ninput_text = prefix + question\ninputs = tokenizer(input_text, return_tensors=\"pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_device = next(model.parameters()).device\n\ninputs = {key: value.to(model_device) for key, value in inputs.items()}\n\n\nimport torch\nmodel.eval()\n\nmodel_device = next(model.parameters()).device\n\ninputs = {key: value.to(model_device) for key, value in inputs.items()}\n\n# Generating the answer\nwith torch.no_grad():\n    outputs = model.generate(inputs[\"input_ids\"], max_length=60, num_beams=2, early_stopping=True)\n\n# Decoding the generated answer\nanswer = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(answer)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}